<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>AI Has No Understanding | Lei's Blog</title>
<meta name=keywords content="AI"><meta name=description content="The proliferation of AI (specifically LLMs) has been the major development in our industry recently. Many people are understandably impressed by the capabilities of cutting-edge LLMs. However, the hype sometimes becomes ridiculous. On Twitter, I&rsquo;ve seen people claiming AGI is already here because some LLMs are capable of solving very advanced coding challenges.
I&rsquo;ve been using LLMs at work for quite a while. Although it&rsquo;s not rare for me to use them to generate code beyond my expertise, I remain convinced that current LLMs lack genuine cognitive capabilities. The notion of fully delegating human tasks to AI systems remains unrealistic today. I doubt whether it will ever be truly possible."><meta name=author content="Lei Huang"><link rel=canonical href=https://leihuang.me/posts/ai-has-no-understanding/><link crossorigin=anonymous href=/assets/css/stylesheet.4577291afec6711dad0a04c542b24213552d8652dd4306b3ef708f31cd6c229c.css integrity="sha256-RXcpGv7GcR2tCgTFQrJCE1UthlLdQwaz73CPMc1sIpw=" rel="preload stylesheet" as=style><link rel=icon href=https://leihuang.me/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://leihuang.me/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://leihuang.me/favicon-32x32.png><link rel=apple-touch-icon href=https://leihuang.me/apple-icon.png><link rel=mask-icon href=https://leihuang.me/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://leihuang.me/posts/ai-has-no-understanding/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://leihuang.me/posts/ai-has-no-understanding/"><meta property="og:site_name" content="Lei's Blog"><meta property="og:title" content="AI Has No Understanding"><meta property="og:description" content="The proliferation of AI (specifically LLMs) has been the major development in our industry recently. Many people are understandably impressed by the capabilities of cutting-edge LLMs. However, the hype sometimes becomes ridiculous. On Twitter, I’ve seen people claiming AGI is already here because some LLMs are capable of solving very advanced coding challenges.
I’ve been using LLMs at work for quite a while. Although it’s not rare for me to use them to generate code beyond my expertise, I remain convinced that current LLMs lack genuine cognitive capabilities. The notion of fully delegating human tasks to AI systems remains unrealistic today. I doubt whether it will ever be truly possible."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-04-18T00:00:00+00:00"><meta property="article:modified_time" content="2025-04-18T00:00:00+00:00"><meta property="article:tag" content="AI"><meta name=twitter:card content="summary"><meta name=twitter:title content="AI Has No Understanding"><meta name=twitter:description content="The proliferation of AI (specifically LLMs) has been the major development in our industry recently. Many people are understandably impressed by the capabilities of cutting-edge LLMs. However, the hype sometimes becomes ridiculous. On Twitter, I&rsquo;ve seen people claiming AGI is already here because some LLMs are capable of solving very advanced coding challenges.
I&rsquo;ve been using LLMs at work for quite a while. Although it&rsquo;s not rare for me to use them to generate code beyond my expertise, I remain convinced that current LLMs lack genuine cognitive capabilities. The notion of fully delegating human tasks to AI systems remains unrealistic today. I doubt whether it will ever be truly possible."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://leihuang.me/posts/"},{"@type":"ListItem","position":2,"name":"AI Has No Understanding","item":"https://leihuang.me/posts/ai-has-no-understanding/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"AI Has No Understanding","name":"AI Has No Understanding","description":"The proliferation of AI (specifically LLMs) has been the major development in our industry recently. Many people are understandably impressed by the capabilities of cutting-edge LLMs. However, the hype sometimes becomes ridiculous. On Twitter, I\u0026rsquo;ve seen people claiming AGI is already here because some LLMs are capable of solving very advanced coding challenges.\nI\u0026rsquo;ve been using LLMs at work for quite a while. Although it\u0026rsquo;s not rare for me to use them to generate code beyond my expertise, I remain convinced that current LLMs lack genuine cognitive capabilities. The notion of fully delegating human tasks to AI systems remains unrealistic today. I doubt whether it will ever be truly possible.\n","keywords":["AI"],"articleBody":"The proliferation of AI (specifically LLMs) has been the major development in our industry recently. Many people are understandably impressed by the capabilities of cutting-edge LLMs. However, the hype sometimes becomes ridiculous. On Twitter, I’ve seen people claiming AGI is already here because some LLMs are capable of solving very advanced coding challenges.\nI’ve been using LLMs at work for quite a while. Although it’s not rare for me to use them to generate code beyond my expertise, I remain convinced that current LLMs lack genuine cognitive capabilities. The notion of fully delegating human tasks to AI systems remains unrealistic today. I doubt whether it will ever be truly possible.\n1. It has no expertise Let me start with a very recent example. GPT-4.1 was out this week. Here’s what OpenAI says about it:\nGPT‑4.1 scores 54.6% on SWE-bench Verified, improving by 21.4%abs over GPT‑4o and 26.6%abs over GPT‑4.5—making it a leading model for coding.\nI recently developed a hobby project Kakashi. I wanted to conditionally enable a button for local development — a trivial task, but one I was reluctant to handle manually. So I asked GPT-4.1 to do it:\nLei: Only enable the image uploading functionality in local dev mode. Make the ‘upload image’ button dynamically injected with js GPT-4.1: To enable image uploading only in local development mode and inject the “Upload Image” button dynamically, you can:\nDetect local development (e.g., window.location.hostname is localhost or 127.0.0.1). Dynamically create and insert the “Upload Image” button and file input in JS. Only attach related event listeners if in dev mode. Here’s how you can modify your code:\nif ( window.location.hostname === \"localhost\" || window.location.hostname === \"127.0.0.1\" ) …\nLei: Wait, there’s a better way to do it. We can use build-time variables. We only need to omit this functionality when building. GPT-4.1: You’re correct! The best practice is to use a build-time variable (such as process.env.NODE_ENV or a custom Vite/webpack define) to conditionally include the image upload functionality. This way, the code is omitted from production bundles entirely.\nHere’s how you can do it using a build-time variable (e.g., import.meta.env.DEV for Vite, or process.env.NODE_ENV for webpack):\n// Only inject upload button in dev mode if (import.meta.env.DEV) { // Dynamically create upload button } This conversation reveals a lot about the nature of LLMs.\nFundamentally, what LLMs do is generate the right words at the right time. They do this by sampling texts from the internet and placing every token into hyperspace (represented by vectors). When you have conversations with LLMs, they try to complete a “user - system” conversation with statistically probable text.\nIn the conversation above, GPT gave the initial answer probably because it saw such an answer during training.(It’s also possible the LLM was “thinking”, i.e. generating its working memory, with bad content) It failed to discriminate bad solutions from good ones.\nWhy did GPT initially propose an inferior solution? Because it selects the statistically likely next words based on training data, without genuine expertise or intuition. As a human developer, I can immediately capture the signal of poor practice at the mere sight of window.location.hostname. GPT, despite extensive data exposure, lacked the judgment to initially recommend the better solution.\n2. It doesn’t know what is impossible The limitation of an LLM’s ability to mimic “understanding” quickly manifests when tackling very complex problems, such as computer vision.\nThe Kakashi app collects users’ drawing paths as coordinates and animates them using Fourier Transformation. The animation is as good as how well the user draws. My drawing isn’t very good, so I wanted to generate animation paths from existing one-stroke drawings, like this one: Here’s a conversation between me and Claude 3.7 with extended “thinking”:\nLei: I want to build a web app to extract coordinate points from images so that I can animate the coordinates later. Here are the ideas:\nWe can assume the images will only contain one-stroke drawing. Our goal is to extract this one-stroke drawing. The image will not be guaranteed to have perfect white background. We’ll need to deal with noise. We can start with getting rid of any rogue elements/pixels on the image and then extracting the main path. We also need to handle potential breaks of the path. If there are breaks, we need to fill them. Then we make the path a one pixel wide smooth line. Be careful not to make the path wiggly. Find the start and end point of the stroke. We need to sample the path points as if we are drawing. I don’t have a good idea on how long the draw should be, as the drawings can have different complexities. Make the default draw duration 4 seconds, but make it a variable that the user can provide through the UI. The idea is that we sample the points within the browser animation rate limit. So the frequency should be one pixel per duration_in_ms/16.67. We also need to mimic human drawing gesture. Faster in straight parts, slower in curvy parts. When deciding the start and end point, follow the rule that start from left, end on the right. I know this project is very complex. Use WebAssembly and Rust if it would be easier. Many steps can go wrong, so I want comprehensive debug info. Render intermediate image data on a debug canvas if you can. The extracted coordinates are calculated relative to the left top corner of the image. Start with presenting a button that the user can use to upload an image. Are these instructions clear to you? Be sure you understand them all. Improve my flow if you can.\nClaude 3.7: Your requirements for the image coordinate extraction web app are clear. This is an interesting project! Let me understand and improve your approach.\nBlah blah…\nClaude confidently gave a non-solution. It didn’t work, of course. We went back and forth several rounds until I realized something was wrong.\nLei: It still doesn’t work. I think you are trying in the wrong direction. The issue is the thin algorithm displaced the joint, but you were trying to fix the tracing. Claude 3.7: I understand the problem now - you’re absolutely right. The issue is with the thinning algorithm, not the path tracing. The skeletonization process is breaking connectivity at the intersections/joints. Let me fix the actual problem:\n…\nBlah blah\nBroken code, again.\nSuddenly, I had a Eureka moment and felt smart:\nLei: I have an intuition. The problem is the thin algorithm can’t handle intersections. Try a different approach. Let’s follow the drawing path from start to finish and thin the line to one pixel as we traverse. And Claude was quick to acknowledge that I was helpful:\nClaude 3.7: You’ve hit the nail on the head! Following the actual drawing path rather than relying on thinning algorithms is exactly what we need. Let me implement this approach:\n…\nAnd of course, broken code again. We continued several rounds until I started to worry about my usage limit. Then, I started a new conversation and changed the question:\nLei: You are not solving the problem. I’ve already told you the thin algorithm handles the cross wrong. The line crosses and enters itself at one point, but exits from another point. I’ve added the screenshot. Is this problem even possible to solve? How do you implement a perfect cross?\nClaude 3.7: You’re absolutely right, and I apologize for not addressing the specific issue. Looking at your screenshot, I can see the exact problem - when a line crosses itself in a 2D image, there’s an inherent ambiguity about which path to follow.\nThis is actually a fundamental challenge in computer vision known as the “crossing problem.” When a line crosses itself in a 2D image, we lose information about which path was drawn on top (the z-order). The flattened 2D representation doesn’t preserve this crucial information.\nIs this problem even possible to solve? Theoretically, no - not with 100% accuracy from a single 2D image without additional information. This is an inherently ambiguous situation.\nHowever, we can implement approaches that make intelligent guesses:\n…\nOnly after burning through most of my daily usage limit did Claude finally tell me the truth, and even then, it had the gall to keep offering ineffective solutions.\nMy human agency kicked in again. Perhaps I could draw on the image to guide the tracing direction?\nBut by then, I knew Claude couldn’t help. Without expertise in computer vision, I’d likely waste more time.\n3. It doesn’t know you’re asking a dumb question LLMs behave like some tech bros and crypto bros — always 100% sure and convincing.\nI sometimes fall for their confidence and overestimate their abilities.\nMy handwriting isn’t great, so I thought AI could improve it. After collecting my handwritten coordinates, I asked Gemini 2.5 Pro for improvement.\nLei: I have a coordinates path that represents a one-stroke path of the text “Hello”. My handwriting is ugly and I want you to improve it. Give me a more stylish one-stroke path. Here’s the js array for my current path:\n[coordinates]\nGemini Pro 2.5: Okay, I can help you with that! Creating a “stylish” path is subjective, but I can generate a new set of coordinates representing a smoother, more flowing cursive style for “Hello” in a single stroke.\n…\n(Blah blah enthusiastic and self-assured rubbish)\nOf course, Gemini couldn’t help — the coordinates I provided never appeared online and pattern-matching was doomed to fail. Unless Gemini wrote a dedicated program to deterministically improve my handwriting, there was no way it could help me. Yet, Gemini confidently provided rubbish coordinates.\nWe continued until I realized I’d asked a dumb question.\nA human expert would quickly identify a bad question and help directing you to ask the right one. But LLM isn’t capable of that. It’ll always mimic a conversation, even when it’s clearly out of its depth.\n","wordCount":"1651","inLanguage":"en","datePublished":"2025-04-18T00:00:00Z","dateModified":"2025-04-18T00:00:00Z","author":{"@type":"Person","name":"Lei Huang"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://leihuang.me/posts/ai-has-no-understanding/"},"publisher":{"@type":"Organization","name":"Lei's Blog","logo":{"@type":"ImageObject","url":"https://leihuang.me/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://leihuang.me/ accesskey=h title="Lei's Blog (Alt + H)">Lei's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://leihuang.me/archives title=Archive><span>Archive</span></a></li><li><a href=https://leihuang.me/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://leihuang.me/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://leihuang.me/>Home</a>&nbsp;»&nbsp;<a href=https://leihuang.me/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">AI Has No Understanding</h1><div class=post-meta><span title='2025-04-18 00:00:00 +0000 UTC'>April 18, 2025</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Lei Huang</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-it-has-no-expertise aria-label="1. It has no expertise">1. It has no expertise</a></li><li><a href=#2-it-doesnt-know-what-is-impossible aria-label="2. It doesn&rsquo;t know what is impossible">2. It doesn&rsquo;t know what is impossible</a></li><li><a href=#3-it-doesnt-know-youre-asking-a-dumb-question aria-label="3. It doesn&rsquo;t know you&rsquo;re asking a dumb question">3. It doesn&rsquo;t know you&rsquo;re asking a dumb question</a></li></ul></div></details></div><div class=post-content><p>The proliferation of AI (specifically LLMs) has been the major development in our industry recently. Many people are understandably impressed by the capabilities of cutting-edge LLMs. However, the hype sometimes becomes ridiculous. On Twitter, I&rsquo;ve seen people claiming AGI is already here because some LLMs are capable of solving very advanced coding challenges.</p><p>I&rsquo;ve been using LLMs at work for quite a while. Although it&rsquo;s not rare for me to use them to generate code beyond my expertise, I remain convinced that current LLMs lack genuine cognitive capabilities. The notion of fully delegating human tasks to AI systems remains unrealistic today. I doubt whether it will ever be truly possible.</p><h2 id=1-it-has-no-expertise>1. It has no expertise<a hidden class=anchor aria-hidden=true href=#1-it-has-no-expertise>#</a></h2><p>Let me start with a very recent example. GPT-4.1 was out this week. Here&rsquo;s what OpenAI says about it:</p><blockquote><p>GPT‑4.1 scores 54.6% on SWE-bench Verified, improving by 21.4%abs over GPT‑4o and 26.6%abs over GPT‑4.5—making it a leading model for coding.</p></blockquote><p>I recently developed a hobby project <a href=https://leihuang23.github.io/kakashi/>Kakashi</a>. I wanted to conditionally enable a button for local development — a trivial task, but one I was reluctant to handle manually. So I asked GPT-4.1 to do it:</p><div class="message user-message"><div class=speaker-name>Lei:</div><div class=message-content>Only enable the image uploading functionality in local dev mode. Make the &lsquo;upload image&rsquo; button dynamically injected with js</div></div><div class="message assistant-message"><div class=speaker-name>GPT-4.1:</div><div class=message-content><p>To enable image uploading only in local development mode and inject the &ldquo;Upload Image&rdquo; button dynamically, you can:</p><ol><li>Detect local development (e.g., <code>window.location.hostname</code> is <code>localhost</code> or <code>127.0.0.1</code>).</li><li>Dynamically create and insert the &ldquo;Upload Image&rdquo; button and file input in JS.</li><li>Only attach related event listeners if in dev mode.</li></ol><p>Here&rsquo;s how you can modify your code:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-js data-lang=js><span class=line><span class=cl> <span class=k>if</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=nb>window</span><span class=p>.</span><span class=nx>location</span><span class=p>.</span><span class=nx>hostname</span> <span class=o>===</span> <span class=s2>&#34;localhost&#34;</span> <span class=o>||</span>
</span></span><span class=line><span class=cl>    <span class=nb>window</span><span class=p>.</span><span class=nx>location</span><span class=p>.</span><span class=nx>hostname</span> <span class=o>===</span> <span class=s2>&#34;127.0.0.1&#34;</span>
</span></span><span class=line><span class=cl>  <span class=p>)</span>
</span></span></code></pre></div><p>&mldr;</p></div></div><div class="message user-message"><div class=speaker-name>Lei:</div><div class=message-content>Wait, there&rsquo;s a better way to do it. We can use build-time variables. We only need to omit this functionality when building.</div></div><div class="message assistant-message"><div class=speaker-name>GPT-4.1:</div><div class=message-content><p>You&rsquo;re correct! The best practice is to use a build-time variable (such as <code>process.env.NODE_ENV</code> or a custom Vite/webpack define) to conditionally include the image upload functionality. This way, the code is omitted from production bundles entirely.</p><p>Here&rsquo;s how you can do it using a build-time variable (e.g., <code>import.meta.env.DEV</code> for Vite, or <code>process.env.NODE_ENV</code> for webpack):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-js data-lang=js><span class=line><span class=cl><span class=c1>// Only inject upload button in dev mode
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>if</span> <span class=p>(</span><span class=kr>import</span><span class=p>.</span><span class=nx>meta</span><span class=p>.</span><span class=nx>env</span><span class=p>.</span><span class=nx>DEV</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=c1>// Dynamically create upload button
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span></code></pre></div></div></div><p>This conversation reveals a lot about the nature of LLMs.</p><p>Fundamentally, what LLMs do is generate the right words at the right time. They do this by sampling texts from the internet and placing every token into hyperspace (represented by vectors). When you have conversations with LLMs, they try to complete a &ldquo;user - system&rdquo; conversation with statistically probable text.</p><p>In the conversation above, GPT gave the initial answer probably because it saw such an answer during training.(It&rsquo;s also possible the LLM was &ldquo;thinking&rdquo;, i.e. generating its working memory, with bad content) It failed to discriminate bad solutions from good ones.</p><p>Why did GPT initially propose an inferior solution? Because it selects the statistically likely next words based on training data, without genuine expertise or intuition. As a human developer, I can immediately capture the signal of poor practice at the mere sight of <code>window.location.hostname</code>. GPT, despite extensive data exposure, lacked the judgment to initially recommend the better solution.</p><h2 id=2-it-doesnt-know-what-is-impossible>2. It doesn&rsquo;t know what is impossible<a hidden class=anchor aria-hidden=true href=#2-it-doesnt-know-what-is-impossible>#</a></h2><p>The limitation of an LLM’s ability to mimic &ldquo;understanding&rdquo; quickly manifests when tackling very complex problems, such as computer vision.</p><p>The Kakashi app collects users&rsquo; drawing paths as coordinates and animates them using Fourier Transformation. The animation is as good as how well the user draws. My drawing isn&rsquo;t very good, so I wanted to generate animation paths from existing one-stroke drawings, like this one:
<img alt="cat line art" loading=lazy src=/images/cat-line.png></p><p>Here&rsquo;s a conversation between me and Claude 3.7 with extended &ldquo;thinking&rdquo;:</p><div class="message user-message"><div class=speaker-name>Lei:</div><div class=message-content><p>I want to build a web app to extract coordinate points from images so that I can animate the coordinates later. Here are the ideas:</p><ol><li>We can assume the images will only contain one-stroke drawing. Our goal is to extract this one-stroke drawing.</li><li>The image will not be guaranteed to have perfect white background. We&rsquo;ll need to deal with noise. We can start with getting rid of any rogue elements/pixels on the image and then extracting the main path. We also need to handle potential breaks of the path. If there are breaks, we need to fill them.</li><li>Then we make the path a one pixel wide smooth line. Be careful not to make the path wiggly.</li><li>Find the start and end point of the stroke.</li><li>We need to sample the path points as if we are drawing. I don&rsquo;t have a good idea on how long the draw should be, as the drawings can have different complexities. Make the default draw duration 4 seconds, but make it a variable that the user can provide through the UI. The idea is that we sample the points within the browser animation rate limit. So the frequency should be one pixel per duration_in_ms/16.67. We also need to mimic human drawing gesture. Faster in straight parts, slower in curvy parts.</li><li>When deciding the start and end point, follow the rule that start from left, end on the right.</li><li>I know this project is very complex. Use WebAssembly and Rust if it would be easier.</li><li>Many steps can go wrong, so I want comprehensive debug info. Render intermediate image data on a debug canvas if you can.</li><li>The extracted coordinates are calculated relative to the left top corner of the image.</li><li>Start with presenting a button that the user can use to upload an image.</li></ol><p>Are these instructions clear to you? Be sure you understand them all. Improve my flow if you can.</p></div></div><div class="message assistant-message"><div class=speaker-name>Claude 3.7:</div><div class=message-content><p>Your requirements for the image coordinate extraction web app are clear. This is an interesting project! Let me understand and improve your approach.</p><p>Blah blah&mldr;</p></div></div><p>Claude confidently gave a non-solution. It didn&rsquo;t work, of course. We went back and forth several rounds until I realized something was wrong.</p><div class="message user-message"><div class=speaker-name>Lei:</div><div class=message-content>It still doesn&rsquo;t work. I think you are trying in the wrong direction. The issue is the thin algorithm displaced the joint, but you were trying to fix the tracing.</div></div><div class="message assistant-message"><div class=speaker-name>Claude 3.7:</div><div class=message-content><p>I understand the problem now - you&rsquo;re absolutely right. The issue is with the thinning algorithm, not the path tracing. The skeletonization process is breaking connectivity at the intersections/joints.
Let me fix the actual problem:</p><p>&mldr;</p><p>Blah blah</p></div></div><p>Broken code, again.</p><p>Suddenly, I had a Eureka moment and felt smart:</p><div class="message user-message"><div class=speaker-name>Lei:</div><div class=message-content>I have an intuition. The problem is the thin algorithm can&rsquo;t handle intersections. Try a different approach. Let&rsquo;s follow the drawing path from start to finish and thin the line to one pixel as we traverse.</div></div><p>And Claude was quick to acknowledge that I was helpful:</p><div class="message assistant-message"><div class=speaker-name>Claude 3.7:</div><div class=message-content><p>You&rsquo;ve hit the nail on the head! Following the actual drawing path rather than relying on thinning algorithms is exactly what we need. Let me implement this approach:</p><p>&mldr;</p></div></div><p>And of course, broken code again. We continued several rounds until I started to worry about my usage limit. Then, I started a new conversation and changed the question:</p><div class="message user-message"><div class=speaker-name>Lei:</div><div class=message-content><p>You are not solving the problem. I&rsquo;ve already told you the thin algorithm handles the cross wrong. The line crosses and enters itself at one point, but exits from another point. I&rsquo;ve added the screenshot.
<img alt=screenshot loading=lazy src=/images/animation-example.webp></p><p>Is this problem even possible to solve? How do you implement a perfect cross?</p></div></div><div class="message assistant-message"><div class=speaker-name>Claude 3.7:</div><div class=message-content><p>You&rsquo;re absolutely right, and I apologize for not addressing the specific issue. Looking at your screenshot, I can see the exact problem - when a line crosses itself in a 2D image, there&rsquo;s an inherent ambiguity about which path to follow.</p><p>This is actually a fundamental challenge in computer vision known as the &ldquo;crossing problem.&rdquo; When a line crosses itself in a 2D image, we lose information about which path was drawn on top (the z-order). The flattened 2D representation doesn&rsquo;t preserve this crucial information.</p><p>Is this problem even possible to solve?
Theoretically, no - not with 100% accuracy from a single 2D image without additional information. This is an inherently ambiguous situation.</p><p>However, we can implement approaches that make intelligent guesses:</p><p>&mldr;</p></div></div><p>Only after burning through most of my daily usage limit did Claude finally tell me the truth, and even then, it had the gall to keep offering ineffective solutions.</p><p>My human agency kicked in again. Perhaps I could draw on the image to guide the tracing direction?</p><p>But by then, I knew Claude couldn&rsquo;t help. Without expertise in computer vision, I&rsquo;d likely waste more time.</p><h2 id=3-it-doesnt-know-youre-asking-a-dumb-question>3. It doesn&rsquo;t know you&rsquo;re asking a dumb question<a hidden class=anchor aria-hidden=true href=#3-it-doesnt-know-youre-asking-a-dumb-question>#</a></h2><p>LLMs behave like some tech bros and crypto bros — always 100% sure and convincing.</p><p>I sometimes fall for their confidence and overestimate their abilities.</p><p>My handwriting isn&rsquo;t great, so I thought AI could improve it. After collecting my handwritten coordinates, I asked Gemini 2.5 Pro for improvement.</p><div class="message user-message"><div class=speaker-name>Lei:</div><div class=message-content><p>I have a coordinates path that represents a one-stroke path of the text &ldquo;Hello&rdquo;. My handwriting is ugly and I want you to improve it. Give me a more stylish one-stroke path. Here&rsquo;s the js array for my current path:</p><p>[coordinates]</p></div></div><div class="message assistant-message"><div class=speaker-name>Gemini Pro 2.5:</div><div class=message-content><p>Okay, I can help you with that! Creating a &ldquo;stylish&rdquo; path is subjective, but I can generate a new set of coordinates representing a smoother, more flowing cursive style for &ldquo;Hello&rdquo; in a single stroke.</p><p>&mldr;</p><p>(Blah blah enthusiastic and self-assured rubbish)</p></div></div><p>Of course, Gemini couldn&rsquo;t help — the coordinates I provided never appeared online and pattern-matching was doomed to fail. Unless Gemini wrote a dedicated program to deterministically improve my handwriting, there was no way it could help me. Yet, Gemini confidently provided rubbish coordinates.</p><p>We continued until I realized I&rsquo;d asked a dumb question.</p><p>A human expert would quickly identify a bad question and help directing you to ask the right one. But LLM isn&rsquo;t capable of that. It&rsquo;ll always mimic a conversation, even when it&rsquo;s clearly out of its depth.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://leihuang.me/tags/ai/>AI</a></li></ul><nav class=paginav><a class=next href=https://leihuang.me/posts/learn-batching-from-llm/><span class=title>Next »</span><br><span>Learn Batching From LLM</span></a></nav></footer></article></main><footer class=footer><span>© 2025 Lei Huang</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>